
[ DNN ]
- 많은 은닉층
- Linear Layer로 입력층 + 은닉층 x N + 출력층
- 주의사항 1) 기울기 손실   2) 과대적합
- 이미지에서 인식률이 떨어진다
- 여기선 이미지를 1차원으로 펼쳐서 넣음
        
==> 펼치지 말고 넣자


[ CNN ]
- : Layer + AF
- 입력층
- 은닉층
- 은닉층
---------------- 여기까지 이미지 그대로 사용
- 은닉층
- 출력층
---------------- 나머지는 1D로 사용

[ RNN ]
: Layer + AF
- CNN 에서와 똑같은 구조에 음성 인식 기능


- 나머지도 층 개수와 모델만 바뀌는 정도다


[ Overfitting ] 과대적합 : 해결 방법
- 노드를 랜덤으로 선정 -> nn.Dropout Layer

[nn.Dropout]
: 학습 시 과대적합을 방지하는 것이므로, drop=True 매개변수로 testing에선 반영 안되도록 한다

class torch.nn.Dropout(p=0.5, inplace=True)
    -  p : 노드 값이 0이 될 (무시할) 확률 (default = 0.5)

class Net(nn.Module):
    def __init__(self):
        """
        초기 신경망 구성
        """
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28*28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        self.dropout_prob = 0.5

    def forward(self, x):
        """
        forward : 학습 진행
        """
        x = x.view(-1, 28*28)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training, p=self.dropout_prob)
        x = F.relu(self.fc2(x))
        x = F.dropout(x, training, training=self.training, p=self.dropout_prob)
        return self.fc3(x)


class DropoutModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.Layer1 = nn.Linear(784, 1200)
        self.dropout1 = nn.Dropout(0.5)
        self.Layer2 = nn.Linear(1200, 1200)     # 1/2은 0이되지만 개수는 그대로 1200
        self.dropout2 = nn.Dropout(0.5)
        self.Layer3 = nn.Linear(1200, 10)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.dropout1(x)
        x = F.relu(self.layer2(x))
        x = self.dropout2(x)
        return self.layer3(x)

ㅁ 만들어진 모델에 과대적합이 날 경우, dropout을 중간에 넣어주는 것





[ Gradient Vanishing & Exploding ]  기울기 손실 및 증폭
1. 활성화 함수 사용 : ReLU, Sigmoid/Softmax
    - ReLU : 0보다 작은 값은 0으로, 0보다 큰 값은 그대로
    - Sigmoid : 0 ~ 1 사이의 값으로 변환
    - Softmax : 0 ~ 1 사이의 값으로 변환, 합이 1이 되도록 변환

2. Normalization -> 배치 정규화
    - nn.BatchNorm1d(채널 수) : 은닉층 출력에 배치 정규화 적용

3. weight initialization
    - 초기 가중치를 적절히 초기화
    - nn.init.xavier_uniform_(self.layer1.weight)
    - nn.init.xavier_uniform_(self.layer2.weight)
    1) Xavier Initialization
    2) Gloret Initialization
    3) He Initialization

4. Gradient Clipping
    - 기울기 폭주를 막기 위해 기울기를 일정 값으로 제한
    - torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

===================================================================

[ 프로젝트 ]
1. 주제 : 팀별로 메인 주제 => 개인 서브 주제
2. 성능평가지표 : 데이터에 따라 결정 (회귀, 분류, 다중분류)
3. 모델 : 층, 층별 노드/퍼셉트론 수
4. 함수 - 전체 흐름
    1) 학습
    2) 검증 및 테스트
    3) 예측
5. 발표자료(주요 내용)
    1) 주제 선정 이유
    2) 탐색 분석 결과 => 분석방법, 평가지표 결정
    3) 모델 => 왜 손실함수, 활성화함수를 선택했는지
6. 주의사항
    - 수업 코드와 같으면 안됨