
[ 딥 러닝 ]
- 뉴런 기반 인공지능

[ CPU / GPU ]
- gpu는 단순한 연산을 여러 코어로 진행 (알지?)

[인공신경망 ANN]
- 뉴런의 구조를 가져옴
- 입력층, 은닉층, 출력층으로 구성
- 자극(입력) 지점(x0)이 가장 강하고, 거리가 멀수록(x1, x2, ...) 자극이 약하다
- 활성 함수 : 입력 값을 출력 시 강화 or 약화를 결정하는 함수
    - 가중치 값을 통해 자극의 영향을 조절할 수 있다.
    - 함수식 : 두 벡터의 내적 + 편향(b)
            
[퍼셉트론 증명]
- OR gate와 AND gate 
: 가중치에 따라 함수(빨간 직선)의 위치가 달라지고 해당하는 (1이 되는) 값이 달라진다.

- OR 게이트 
def or_gate(x1, x2):
    # 입력 값을 가중치 값과 편향 값을 더한다.
    w1, w2 = 0.5, 0.5
    b = 0.5
    value = w1 * x1 + w2 * x2 + b
    return value if value > 0 else 0

- AND 게이트, NAND 게이트

- XOR Gate는 OR과 NAND의 합으로 만들 수 있다. => 여러 단계(층)으로 구성

[다층 퍼셉트론]
- 여러 층이 연결되어 있는 퍼셉트론 : 입력층    은닉층      출력층
- 예시 : OR, NAND = 입력층 / XOR = 출력층 (가장 간단한 구조)
- Q. 많은 층 vs 많은 퍼셉트론
 : 층이 많은 경우가 정확도가 높다

[동작 원리]
 1단계 : 순전파 forward propagation
    - 입력 => 출력층 방향의 계산 과정
    - 신호에 가중치를 곱한 값을 출력층까지 차례대로 계산 (활성화함수)
    - 각각의 입력층 퍼셉트론은 모든 은닉층 퍼셉트론에게 전달해야한다
 
 2단계 : 역전파 backward propagation
    - 출력 => 입력층 방향
    - 출력층에서 오차를 계산하고 은닉층에 전달한다
        - 오류에 대한 미분값, 학습률, 경사하강법 이용한 최적화 값을 연산 => 입력층으로 전달

- 입력 => 출력 => 오차계산 => 입력 => 출력 => 오차계산 ... 반복
    - 오차가 최소화될 때까지 w, b를 업데이트
    - (ML에서) max_iter = 반복하는 횟수!

[활성함수]
- 가중치에 따라 다양한 형태를 가짐 (Step, Sigmoid, ReLu, ...)


[기울기 소실/폭주]
1. 소실
    - 미분 반복 : 결국 기울기(가중치)가 소실된다
2. 폭주
    - 또는 기울기가 커져버림 

=> 해결 방법
1) 활성화 함수 : 활성함수 자체가 소실/폭주를 막기 위한 것
    - 입력/은닉 : ReLu 계열
    - 출력 : Sigmoid/Softmax/Linear

2) 가중치 초기화 
    - 세이비어 어쩌구 : 성능 구림
    - 헤(He) 초기화 : 좋은 성능
        - ReLu + He 초기화 방법이 보편적

3) 정규화
    1. 배치 정규화
        - 배치 내의 특정 특징 값을 평균화/표준편차화
        - 단점 : 배치 사이즈 영향 큼(너무 작으면 안됨), RNN 적용 어려움
    2. 층 정규화
        - 층 단위로 정규화 진행 (배치는 행, 층은 열이라 보기)

# -----------------------------------------------------------------
오후 수업

- ex_linearRegression.ipynb 참고

[선형 회귀 따라가보기]
- 

opitmizer.zero_grad()   # 초기화: 메모리 정리
cost.backward()         # 역전파 : 미분 후 gradient 계산
optimizer.step()        # 최적화: 가중치 업데이트 / w에 저장

=> 이 과정이 LinearRegression() 내에서 일어나는 일

