{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet & DataLoader 살펴보기\n",
    "- pytorch에서 배치크기만큼 데이터를 조절하기 위한 메커니즘\n",
    "- Dataset : 사용자 데이터를 기반으로 사용자 정의 클래스 작성\n",
    "- DataLoad : 지정된 Dataset에서 지정된 배치 크기만큼 피처와 타깃을 추출하여 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) 2 torch.Size([5, 1]) 2\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "x_data = torch.IntTensor(\n",
    "    [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60], [50, 60, 70]]\n",
    ")\n",
    "y_data = torch.FloatTensor([[20], [30], [40], [50], [60]])\n",
    "\n",
    "print(x_data.shape, x_data.ndim, y_data.shape, y_data.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10, 20, 30],\n",
       "         [20, 30, 40],\n",
       "         [30, 40, 50],\n",
       "         [40, 50, 60],\n",
       "         [50, 60, 70]], dtype=torch.int32),\n",
       " tensor([[20.],\n",
       "         [30.],\n",
       "         [40.],\n",
       "         [50.],\n",
       "         [60.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Create DataSet\n",
    "# 1) TensoririsDFset 활용 : Dataset의 sub_class\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "dataset.tensors\n",
    "\n",
    "# 주의 : x, y data의 행 번호가 맞아야 실행된다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10, 20, 30], dtype=torch.int32), tensor([20.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __getitem__() 메서드 호출\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) 사용자 정의 데이터셋 생성\n",
    "# (1) Load file\n",
    "irisDF = pd.read_csv('iris.csv')\n",
    "irisDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) feature : numpy로 가져오기\n",
    "irisNP = np.loadtxt('iris.csv', delimiter=',', usecols=[0, 1, 2, 3], skiprows=1)\n",
    "irisNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) 사용자 정의 Dataset class\n",
    "# - callback function\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):  # 초기화 함수\n",
    "        super().__init__()\n",
    "        x_data = x_data.values if isinstance(x_data, pd.DataFrame) else x_data  \n",
    "        y_data = y_data.values if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        # : x_data, y_data가 DataFrame이면 value를 반환\n",
    "               \n",
    "        # ndarray ==> Tensor\n",
    "        self.feature = torch.FloatTensor(x_data)\n",
    "        self.target = torch.LongTensor(y_data)\n",
    "        print(f'[feature&target shape] feature :{self.feature.shape}, target : {self.target.shape}')\n",
    "\n",
    "    def __len__(self):  # 갯수 확인 콜백 함수\n",
    "        return self.target.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):   # 특정 인덱스 데이터+라벨 반환 콜백 함수\n",
    "        return self.feature[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "DataFrame\n",
      "ndarray\n"
     ]
    }
   ],
   "source": [
    "# check datatype\n",
    "print(\n",
    "    type(irisDF), \n",
    "    type(irisNP),\n",
    "    irisDF.__class__.__name__,\n",
    "    irisNP.__class__.__name__,\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# Split feature and target\n",
    "featureDF, targetDF = irisDF[irisDF.columns[:-1]], irisDF[irisDF.columns[-1]]\n",
    "print(featureDF.shape, targetDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize target\n",
    "targetDF = targetDF.replace({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n",
    "targetDF.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy에서 차원 증가 = reshape()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "targetNP = LabelEncoder().fit_transform(targetDF)\n",
    "targetNP.shape  # 차원\n",
    "targetNP = targetNP.reshape(-1, 1)\n",
    "targetNP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 생성 IrisDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor([0.])),\n",
       " sepal_length    5.1\n",
       " sepal_width     3.5\n",
       " petal_length    1.4\n",
       " petal_width     0.2\n",
       " Name: 0, dtype: float64,\n",
       " 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "my_dataset = IrisDataset(featureDF, targetNP)\n",
    "my_dataset[0], featureDF.iloc[0], targetDF[0]   # 첫 값을 반환\n",
    "# : dataset을 만들었을 때 해당 인덱스를 튜플로 반환, 이를 DF로 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===> dataset 만듦을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>오후 수업<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_split from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds: 105개, val_ds: 15개, test_ds: 30개\n",
      "train_ds의 Subset 속성 :\n",
      "    indices : [141, 44, 66, 56, 17, 122, 83, 75, 101, 41, 92, 98, 89, 102, 30, 90, 130, 86, 94, 12, 58, 61, 34, 24, 138, 128, 95, 124, 96, 109, 145, 115, 38, 100, 133, 33, 7, 65, 40, 125, 79, 11, 16, 60, 55, 143, 63, 74, 116, 108, 77, 68, 67, 36, 93, 1, 137, 112, 4, 139, 26, 18, 22, 47, 105, 123, 76, 87, 31, 73, 70, 37, 118, 14, 107, 127, 146, 39, 20, 48, 69, 0, 103, 23, 15, 129, 82, 6, 42, 121, 114, 5, 59, 62, 134, 21, 57, 3, 142, 136, 117, 131, 53, 10, 81]\n",
      "    dataset : <__main__.IrisDataset object at 0x000001EF06871D30>\n"
     ]
    }
   ],
   "source": [
    "# 2-3. DataSet for Training, Valuate, Test\n",
    "# 1) pytorch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set rate of train, val, test\n",
    "seed = torch.Generator().manual_seed(11)\n",
    "train_ds, val_ds, test_ds = random_split(my_dataset, [0.7, 0.1, 0.2], generator=seed)\n",
    "print(\n",
    "    f\"train_ds: {len(train_ds)}개, val_ds: {len(val_ds)}개, test_ds: {len(test_ds)}개\"\n",
    ")\n",
    "print(\n",
    "    f\"\"\"train_ds의 Subset 속성 :\n",
    "    indices : {train_ds.indices}\n",
    "    dataset : {train_ds.dataset}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. DataLoader 생성 : 학습, 검증, 평가용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 3, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-1. Create DataLoader\n",
    "# - drop_last=bool : 배치 사이즈에서 남는 데이터 처리 방법 (false)\n",
    "batch = 5\n",
    "train_dl = DataLoader(train_ds, batch_size=batch)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch)\n",
    "\n",
    "len(train_dl), len(val_dl), len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size : 5\n",
      "train_ds : 105개, val_ds : 15개, test_ds : 30개\n",
      "train_dl : 21개, val_dl : 3개, test_dl : 6개\n"
     ]
    }
   ],
   "source": [
    "# Iteration : Epoch당 반복 단위\n",
    "print(f'batch size : {batch}')\n",
    "print(f'train_ds : {len(train_ds)}개, val_ds : {len(val_ds)}개, test_ds : {len(test_ds)}개')\n",
    "print(f'train_dl : {len(train_dl)}개, val_dl : {len(val_dl)}개, test_dl : {len(test_dl)}개')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] feature : tensor([[6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [5.1000, 3.8000, 1.9000, 0.4000],\n",
      "        [5.6000, 3.0000, 4.5000, 1.5000],\n",
      "        [6.3000, 3.3000, 4.7000, 1.6000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.3000]]), target : tensor([[2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "[1] feature : tensor([[7.7000, 2.8000, 6.7000, 2.0000],\n",
      "        [6.0000, 2.7000, 5.1000, 1.6000],\n",
      "        [6.6000, 3.0000, 4.4000, 1.4000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [4.5000, 2.3000, 1.3000, 0.3000]]), target : tensor([[2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.]])\n",
      "[2] feature : tensor([[5.8000, 2.6000, 4.0000, 1.2000],\n",
      "        [5.1000, 2.5000, 3.0000, 1.1000],\n",
      "        [5.5000, 2.5000, 4.0000, 1.3000],\n",
      "        [7.1000, 3.0000, 5.9000, 2.1000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000]]), target : tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [0.]])\n",
      "[3] feature : tensor([[5.5000, 2.6000, 4.4000, 1.2000],\n",
      "        [7.4000, 2.8000, 6.1000, 1.9000],\n",
      "        [6.7000, 3.1000, 4.7000, 1.5000],\n",
      "        [5.6000, 2.7000, 4.2000, 1.3000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.1000]]), target : tensor([[1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "[4] feature : tensor([[6.6000, 2.9000, 4.6000, 1.3000],\n",
      "        [5.9000, 3.0000, 4.2000, 1.5000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [4.8000, 3.4000, 1.9000, 0.2000],\n",
      "        [6.0000, 3.0000, 4.8000, 1.8000]]), target : tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[5] feature : tensor([[6.4000, 2.8000, 5.6000, 2.1000],\n",
      "        [5.7000, 3.0000, 4.2000, 1.2000],\n",
      "        [6.7000, 3.3000, 5.7000, 2.1000],\n",
      "        [5.7000, 2.9000, 4.2000, 1.3000],\n",
      "        [7.2000, 3.6000, 6.1000, 2.5000]]), target : tensor([[2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "[6] feature : tensor([[6.7000, 3.0000, 5.2000, 2.3000],\n",
      "        [6.4000, 3.2000, 5.3000, 2.3000],\n",
      "        [4.4000, 3.0000, 1.3000, 0.2000],\n",
      "        [6.3000, 3.3000, 6.0000, 2.5000],\n",
      "        [6.3000, 2.8000, 5.1000, 1.5000]]), target : tensor([[2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "[7] feature : tensor([[5.5000, 4.2000, 1.4000, 0.2000],\n",
      "        [5.0000, 3.4000, 1.5000, 0.2000],\n",
      "        [6.7000, 3.1000, 4.4000, 1.4000],\n",
      "        [5.0000, 3.5000, 1.3000, 0.3000],\n",
      "        [7.2000, 3.2000, 6.0000, 1.8000]]), target : tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[8] feature : tensor([[5.7000, 2.6000, 3.5000, 1.0000],\n",
      "        [4.8000, 3.4000, 1.6000, 0.2000],\n",
      "        [5.4000, 3.9000, 1.3000, 0.4000],\n",
      "        [5.0000, 2.0000, 3.5000, 1.0000],\n",
      "        [5.7000, 2.8000, 4.5000, 1.3000]]), target : tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "[9] feature : tensor([[6.8000, 3.2000, 5.9000, 2.3000],\n",
      "        [6.1000, 2.9000, 4.7000, 1.4000],\n",
      "        [6.4000, 2.9000, 4.3000, 1.3000],\n",
      "        [6.5000, 3.0000, 5.5000, 1.8000],\n",
      "        [6.7000, 2.5000, 5.8000, 1.8000]]), target : tensor([[2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "[10] feature : tensor([[6.7000, 3.0000, 5.0000, 1.7000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
      "        [5.5000, 3.5000, 1.3000, 0.2000],\n",
      "        [5.0000, 2.3000, 3.3000, 1.0000]]), target : tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "[11] feature : tensor([[4.9000, 3.0000, 1.4000, 0.2000],\n",
      "        [6.4000, 3.1000, 5.5000, 1.8000],\n",
      "        [6.8000, 3.0000, 5.5000, 2.1000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000],\n",
      "        [6.9000, 3.1000, 5.4000, 2.1000]]), target : tensor([[0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[12] feature : tensor([[5.0000, 3.4000, 1.6000, 0.4000],\n",
      "        [5.7000, 3.8000, 1.7000, 0.3000],\n",
      "        [4.6000, 3.6000, 1.0000, 0.2000],\n",
      "        [4.6000, 3.2000, 1.4000, 0.2000],\n",
      "        [7.6000, 3.0000, 6.6000, 2.1000]]), target : tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[13] feature : tensor([[6.3000, 2.7000, 4.9000, 1.8000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000],\n",
      "        [6.3000, 2.3000, 4.4000, 1.3000],\n",
      "        [5.4000, 3.4000, 1.5000, 0.4000],\n",
      "        [6.1000, 2.8000, 4.7000, 1.2000]]), target : tensor([[2.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "[14] feature : tensor([[5.9000, 3.2000, 4.8000, 1.8000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [7.7000, 2.6000, 6.9000, 2.3000],\n",
      "        [5.8000, 4.0000, 1.2000, 0.2000],\n",
      "        [7.3000, 2.9000, 6.3000, 1.8000]]), target : tensor([[1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[15] feature : tensor([[6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [6.3000, 2.5000, 5.0000, 1.9000],\n",
      "        [5.1000, 3.4000, 1.5000, 0.2000],\n",
      "        [5.4000, 3.4000, 1.7000, 0.2000],\n",
      "        [5.3000, 3.7000, 1.5000, 0.2000]]), target : tensor([[2.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "[16] feature : tensor([[5.6000, 2.5000, 3.9000, 1.1000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [6.3000, 2.9000, 5.6000, 1.8000],\n",
      "        [5.1000, 3.3000, 1.7000, 0.5000],\n",
      "        [5.7000, 4.4000, 1.5000, 0.4000]]), target : tensor([[1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "[17] feature : tensor([[7.2000, 3.0000, 5.8000, 1.6000],\n",
      "        [5.8000, 2.7000, 3.9000, 1.2000],\n",
      "        [4.6000, 3.4000, 1.4000, 0.3000],\n",
      "        [4.4000, 3.2000, 1.3000, 0.2000],\n",
      "        [5.6000, 2.8000, 4.9000, 2.0000]]), target : tensor([[2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [2.]])\n",
      "[18] feature : tensor([[5.8000, 2.8000, 5.1000, 2.4000],\n",
      "        [5.4000, 3.9000, 1.7000, 0.4000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000],\n",
      "        [6.0000, 2.2000, 4.0000, 1.0000],\n",
      "        [6.1000, 2.6000, 5.6000, 1.4000]]), target : tensor([[2.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "[19] feature : tensor([[5.1000, 3.7000, 1.5000, 0.4000],\n",
      "        [4.9000, 2.4000, 3.3000, 1.0000],\n",
      "        [4.6000, 3.1000, 1.5000, 0.2000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [6.3000, 3.4000, 5.6000, 2.4000]]), target : tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.]])\n",
      "[20] feature : tensor([[7.7000, 3.8000, 6.7000, 2.2000],\n",
      "        [7.9000, 3.8000, 6.4000, 2.0000],\n",
      "        [5.5000, 2.3000, 4.0000, 1.3000],\n",
      "        [5.4000, 3.7000, 1.5000, 0.2000],\n",
      "        [5.5000, 2.4000, 3.7000, 1.0000]]), target : tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Attributes of DataLoader\n",
    "for _, (feature, target) in enumerate(train_dl):\n",
    "    print(f'[{_}] feature : {feature}, target : {target}')\n",
    "    # 로더에서 가져온 데이터만큼 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model 클래스 정의 : 입출력 피처 수, 층 수, 은닉 층의 노드 수 <hr>\n",
    "- 구조 설계\n",
    "1) 입력층 : 입력 => 피쳐 개수, iris는 4개\n",
    "2) 은닉층 : 맘대루\n",
    "3) 출력층 : 출력 => [분류] 타겟 클래스 갯수 [회귀] 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델 : 내가 직접 만들어 보기\n",
    "class ClassifyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyModel, self).__init__()\n",
    "        # 모델 레이어를 정의하고 초기화\n",
    "        self.layer1 = nn.Linear(4, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(10, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(5, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 순전파 동작 구현\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "class RegressModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressModule, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(4, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수업 내용 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  모델 클래스 정의\n",
    "# 클래스명 : Classfy_Model\n",
    "class Classfy_Model(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_dim, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layer = nn.Linear(20, 10)\n",
    "        self.output_layer = nn.Linear(10, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Process\n",
    "\n",
    "        Args:\n",
    "            x (_type_): \n",
    "\n",
    "        Returns:\n",
    "            y: After forward process\n",
    "        \"\"\"\n",
    "        \n",
    "        y = self.input_layer(x)     # W1x1+W2x2+...+Wnxn+b 20개 반환\n",
    "        self.relu(y)                # relu() 결과 20개 반환\n",
    "        y = self.hidden_layer(y)    # W1x1+W2x2+...+Wnxn+b 10개 반환\n",
    "        self.relu(y)                # relu() 10개 반환\n",
    "        y = self.output_layer(y)    # W1x1+W2x2+ ... 3개 반환\n",
    "        \n",
    "        return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 학습 준비  \n",
    ": 실행 디바이스, 모델, 최적화, 손실함수, 학습 횟수, 학습함수, 평가함수, 예측함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Classfy_Model(\n",
      "  (input_layer): Linear(in_features=4, out_features=20, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (hidden_layer): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1) Set Device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2) Train Counts\n",
    "EPOCHS = 50\n",
    "\n",
    "# 3) Model instance\n",
    "IN, OUT = my_dataset.feature.shape[1], my_dataset.target.shape[1]  # 강사님: len(np.unique(target_df)) or .nunique()\n",
    "model = Classfy_Model(IN, OUT).to(DEVICE)\n",
    "print(IN, OUT)\n",
    "print(model)\n",
    "\n",
    "# 4) Loss Func.\n",
    "LOSS_FUNC = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 5) 최적화 인스턴스\n",
    "import torch.optim as optim\n",
    "OPTIMIZER = optim.Adam(model.parameters())\n",
    "\n",
    "# 6) Scheduler\n",
    "# - Adjust Learning Rate!\n",
    "from torch.optim .lr_scheduler import ReduceLROnPlateau\n",
    "SCHEDULER = ReduceLROnPlateau(OPTIMIZER, mode='min', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 및 검증 관련 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.functional as metrics\n",
    "\n",
    "# Train Func.\n",
    "def training():\n",
    "    # Training mode\n",
    "    # : 정규화, 경사하강법, 드랍아웃 등의 기능 활성화\n",
    "    model.train()\n",
    "    \n",
    "    # 배치 크기만큼 학습 진행\n",
    "    train_loss, train_acc = [], []\n",
    "    for cnt, (feature, target) in enumerate(train_dl):\n",
    "        # print(cnt, feature, target)\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)      # device setting\n",
    "        target = target.squeeze()\n",
    "        \n",
    "        # 학습\n",
    "        pre_target = model(feature)     # 예측값\n",
    "        \n",
    "        # 손실계산\n",
    "        loss = LOSS_FUNC(pre_target, target)\n",
    "        train_loss.append(loss)\n",
    "        \n",
    "        # W,b 업데이트\n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        \n",
    "        # 정확도 계산\n",
    "        acc = metrics.accuracy(pre_target, target, task='multiclass', num_classes=5)\n",
    "        print(acc)\n",
    "\n",
    "        # message by 'batch'\n",
    "        # print(f'[Train {cnt} batch Loss] ==> {loss}')\n",
    "        \n",
    "    # Print Message by 'Epoch'\n",
    "    print(f'[Train Loss] ==> {loss}')\n",
    "    \n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Func.\n",
    "def testing(dataLoader):\n",
    "    # 추론 모드\n",
    "    # : 정규화, 경사하강법, 드랍아웃 등의 기능 비활성화\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():   # with? : \n",
    "        # Training by size of batch\n",
    "        val_loss = []\n",
    "        for cnt, (feature, target) in enumerate(dataLoader):\n",
    "            # - dataLoader : Loader for Test or Training\n",
    "            \n",
    "            # Set training data by size of batch\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            # Training\n",
    "            pre_target = model(feature)\n",
    "            \n",
    "            # Calc. loss\n",
    "            loss = LOSS_FUNC(pre_target, target.long())\n",
    "\n",
    "            val_loss.append(loss)\n",
    "    \n",
    "    # Print message per Epoch\n",
    "    acc = metrics.accuracy(pre_target, target, task='multiclass', num_classes=3)\n",
    "    print(f'[Valid loss] --> {loss}')\n",
    "    print(f'[Accuarcy]   --> {acc}')\n",
    "            \n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prdiction Func.\n",
    "def predict():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Loss] ==> -0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m training()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Train : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Valid : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(valid_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(valid_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[56], line 21\u001b[0m, in \u001b[0;36mtesting\u001b[1;34m(dataLoader)\u001b[0m\n\u001b[0;32m     18\u001b[0m         pre_target \u001b[38;5;241m=\u001b[39m model(feature)\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Calc. loss\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mLOSS_FUNC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         val_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Print message per Epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "for eps in range(EPOCHS):\n",
    "    # Training\n",
    "    train_loss = training()\n",
    "    \n",
    "    # Validate\n",
    "    valid_loss = testing(val_dl)\n",
    "    \n",
    "    print(f'[{eps}/{EPOCHS}] Train : {sum(train_loss)/len(train_loss)}, Valid : {sum(valid_loss)/len(valid_loss)}')\n",
    "    \n",
    "    # Early stop System: if valid_loss does not improve more than 5 times in a row\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.functional as metrics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
