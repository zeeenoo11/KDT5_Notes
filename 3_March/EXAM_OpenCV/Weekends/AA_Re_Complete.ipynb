{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "train_dir = '../DATA/lang_data/train/'\n",
    "test_dir = '../DATA/lang_data/test/'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load Data / Preprocess Data into Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. test_df_list\n",
    "def dataframe_func(file_dir):\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        for file in files:\n",
    "            f = open(os.path.join(root, file), 'r')\n",
    "            data = f.read()\n",
    "            f.close()\n",
    "            \n",
    "            data = data.split()\n",
    "            data_num = [list(map(ord, list(word.lower()))) for word in data if word.isalpha()]\n",
    "            data_list.append(data_num)\n",
    "            \n",
    "            if 'en' in file:\n",
    "                label_list.append(0)\n",
    "            elif 'fr' in file:\n",
    "                label_list.append(1)\n",
    "            elif 'id' in file:\n",
    "                label_list.append(2)\n",
    "            elif 'tl' in file:\n",
    "                label_list.append(3)\n",
    "\n",
    "    # len_col, len_row = max([len(max(data, key = len)) for data in data_list]), max([len(data) for data in data_list])\n",
    "    # 길이는 우선 지정하자 : col = 45, row = 16100\n",
    "    len_col, len_row = 45, 16100\n",
    "    df_list = []\n",
    "    for idx, data in enumerate(data_list):\n",
    "        data_frame = pd.DataFrame(0, index = range(len_row), columns = range(len_col))\n",
    "        \n",
    "        for idx2, word in enumerate(data):\n",
    "            for idx3, i in enumerate(word):\n",
    "                data_frame.loc[idx2, idx3] = i\n",
    "        \n",
    "        df_list.append(data_frame.values.reshape(-1))   # 1차원으로 reshape, list에 추가\n",
    "    \n",
    "    return df_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이를 우선 최대 길이로 train, test 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) dataset class 생성 : __len__, __getitem__ 구현\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(LangDataset, self).__init__()\n",
    "        self.data = torch.tensor(np.array(x), dtype = torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype = torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data, label 생성\n",
    "train_data, train_label = dataframe_func(train_dir)\n",
    "test_data, test_label = dataframe_func(test_dir)\n",
    "\n",
    "# 2. dataset 생성\n",
    "train_dataset = LangDataset(train_data, train_label)\n",
    "test_dataset = LangDataset(test_data, test_label)\n",
    "\n",
    "# 3. dataloader 생성\n",
    "Batchs = 8\n",
    "generator = torch.Generator().manual_seed(11)\n",
    "train_loader = DataLoader(train_dataset, batch_size = Batchs, shuffle = True, generator = generator)\n",
    "test_loader = DataLoader(test_dataset, batch_size = Batchs, shuffle = True, generator = generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 생성\n",
    "# - basic하게, relu, linear만 사용\n",
    "# - hidden_layer 변수로 조절\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LangModel(nn.Module):\n",
    "    def __init__(self, IN, Hidden_list, OUT):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.in_layer = nn.Linear(IN, Hidden_list[0])\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for idx, hidden in enumerate(Hidden_list[:-1]):\n",
    "            self.hidden.append(nn.Linear(hidden, Hidden_list[idx+1]))\n",
    "        self.out_layer = nn.Linear(Hidden_list[-1], OUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.in_layer(x))   # IN -> Hidden_list[0]\n",
    "        for hidden in self.hidden:\n",
    "            y = F.relu(hidden(y)) # Hidden_list[0] -> Hidden_list[1] -> ... -> Hidden_list[-1]\n",
    "        y = self.out_layer(y)           # Hidden_list[-1] -> OUT\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 / 평가 종합 함수\n",
    "# TODO : .train(), .eval() 적용한 함수 구현\n",
    "# - training 구성 요소 : .train(), for batch\n",
    "\n",
    "def train_eval(model, train_dl, test_dl, Epochs, lr = 0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    for epoch in range(1, Epochs+1):\n",
    "        # training\n",
    "        model.train()\n",
    "        for inputs, labels in train_dl.dataset:\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, 4)   # shape 맞추기\n",
    "            labels = labels.view(-1)        # shape 맞추기\n",
    "            # print(outputs, labels, outputs.shape, labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch : {epoch}, final Loss : {loss}')\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.1, patience = 5)\n",
    "        scheduler.step(loss)   # loss가 감소하지 않으면 학습률을 줄임\n",
    "        # scheduler 정리\n",
    "            # - optimizer : 최적화할 optimizer\n",
    "            # - mode : min / max 중 하나 선택, min: 감소, max: 증가\n",
    "            # - factor : 학습률을 줄일 비율; new_lr = lr * factor\n",
    "            # - patience : 성능이 증가하지 않는 epoch 수\n",
    "            # - verbose : True로 설정하면 감소되는 학습률 출력\n",
    "            \n",
    "        if scheduler.num_bad_epochs > scheduler.patience:\n",
    "            print('Early Stopping : Over patience limit')\n",
    "            break\n",
    "            \n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean_acc, mean_f1 = 0, 0\n",
    "        for idx, (inputs, labels) in enumerate(test_dl.dataset):\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, 4)\n",
    "            labels = labels.view(-1)\n",
    "            pred = torch.argmax(outputs, 1)\n",
    "            acc = accuracy_score(labels, pred)\n",
    "            f1 = f1_score(labels, pred, average = 'macro')\n",
    "            print(f'Prediction : {pred}, Label : {labels}')\n",
    "            print(f'Accuracy : {acc}, F1 : {f1}')\n",
    "            mean_acc += acc\n",
    "            mean_f1 += f1\n",
    "            \n",
    "    \n",
    "    print('Finished Training')\n",
    "    print(f'Epoch : {epoch}, Loss : {loss:.4f}, Total Accuracy : {mean_acc/len(test_dl.dataset)}, Total F1 : {mean_f1/len(test_dl.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((724500,), (724500,), 724500)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape, test_data[0].shape, len(train_data[0])\n",
    "# 아 이거 shape 맞춰야 하네\n",
    "# 일단 돌려본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, final Loss : 1.4765543937683105\n",
      "Epoch : 2, final Loss : 1.4773564338684082\n",
      "Epoch : 3, final Loss : 1.476163625717163\n",
      "Epoch : 4, final Loss : 1.474111795425415\n",
      "Epoch : 5, final Loss : 1.4720945358276367\n",
      "Epoch : 6, final Loss : 1.4700841903686523\n",
      "Epoch : 7, final Loss : 1.4680943489074707\n",
      "Epoch : 8, final Loss : 1.466133713722229\n",
      "Epoch : 9, final Loss : 1.464207410812378\n",
      "Epoch : 10, final Loss : 1.462318778038025\n",
      "Epoch : 11, final Loss : 1.4604696035385132\n",
      "Epoch : 12, final Loss : 1.4586613178253174\n",
      "Epoch : 13, final Loss : 1.4568941593170166\n",
      "Epoch : 14, final Loss : 1.4551684856414795\n",
      "Epoch : 15, final Loss : 1.4534841775894165\n",
      "Epoch : 16, final Loss : 1.4518406391143799\n",
      "Epoch : 17, final Loss : 1.45023775100708\n",
      "Epoch : 18, final Loss : 1.4486744403839111\n",
      "Epoch : 19, final Loss : 1.447150468826294\n",
      "Epoch : 20, final Loss : 1.445664644241333\n",
      "Epoch : 21, final Loss : 1.4442164897918701\n",
      "Epoch : 22, final Loss : 1.4428050518035889\n",
      "Epoch : 23, final Loss : 1.4414297342300415\n",
      "Epoch : 24, final Loss : 1.4400891065597534\n",
      "Epoch : 25, final Loss : 1.438783049583435\n",
      "Epoch : 26, final Loss : 1.4375102519989014\n",
      "Epoch : 27, final Loss : 1.4362701177597046\n",
      "Epoch : 28, final Loss : 1.4350616931915283\n",
      "Epoch : 29, final Loss : 1.4338841438293457\n",
      "Epoch : 30, final Loss : 1.432736873626709\n",
      "Epoch : 31, final Loss : 1.431618571281433\n",
      "Epoch : 32, final Loss : 1.4305293560028076\n",
      "Epoch : 33, final Loss : 1.4294676780700684\n",
      "Epoch : 34, final Loss : 1.4284331798553467\n",
      "Epoch : 35, final Loss : 1.4274250268936157\n",
      "Epoch : 36, final Loss : 1.4264426231384277\n",
      "Epoch : 37, final Loss : 1.4254851341247559\n",
      "Epoch : 38, final Loss : 1.4245519638061523\n",
      "Epoch : 39, final Loss : 1.423642635345459\n",
      "Epoch : 40, final Loss : 1.4227560758590698\n",
      "Epoch : 41, final Loss : 1.4218922853469849\n",
      "Epoch : 42, final Loss : 1.4210500717163086\n",
      "Epoch : 43, final Loss : 1.4202293157577515\n",
      "Epoch : 44, final Loss : 1.4194291830062866\n",
      "Epoch : 45, final Loss : 1.4186491966247559\n",
      "Epoch : 46, final Loss : 1.417888879776001\n",
      "Epoch : 47, final Loss : 1.4171475172042847\n",
      "Epoch : 48, final Loss : 1.4164247512817383\n",
      "Epoch : 49, final Loss : 1.4157202243804932\n",
      "Epoch : 50, final Loss : 1.415033221244812\n",
      "Prediction : tensor([0]), Label : tensor([0])\n",
      "Accuracy : 1.0, F1 : 1.0\n",
      "Prediction : tensor([0]), Label : tensor([0])\n",
      "Accuracy : 1.0, F1 : 1.0\n",
      "Prediction : tensor([0]), Label : tensor([0])\n",
      "Accuracy : 1.0, F1 : 1.0\n",
      "Prediction : tensor([0]), Label : tensor([0])\n",
      "Accuracy : 1.0, F1 : 1.0\n",
      "Prediction : tensor([0]), Label : tensor([1])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([1])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([1])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([1])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([2])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([2])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([2])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([2])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([3])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([3])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([3])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Prediction : tensor([0]), Label : tensor([3])\n",
      "Accuracy : 0.0, F1 : 0.0\n",
      "Finished Training\n",
      "Epoch : 50, Loss : 1.4150, Total Accuracy : 0.25, Total F1 : 0.25\n"
     ]
    }
   ],
   "source": [
    "IN = len(train_data[0])\n",
    "model = LangModel(IN, [64, 32, 16], 4)\n",
    "train_eval(model, train_loader, test_loader, 50, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 데이터로 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프랑스어로 장문의 아무말 입력\n",
    "new_data = 'Je suis un homme de 30 ans. Français, je suis né à Paris. J\\'aime la musique et le sport. Je suis célibataire. Je suis un homme de 30 ans. Français, je suis né à Paris. J\\'aime la musique et le sport. Je suis célibataire. Je suis un homme de 30 ans. Français, je suis né à Paris. J\\'aime la musique et le sport. Je suis célibataire. Je suis un homme de 30 ans. Français, je suis né à Paris. J\\'aime la musique et le sport. Je suis célibataire.'\n",
    "\n",
    "id_data = 'Saya seorang pria berusia 30 tahun. Saya lahir di Jakarta. Saya suka musik dan olahraga. Saya lajang. Saya seorang pria berusia 30 tahun. Saya lahir di Jakarta. Saya suka musik dan olahraga. Saya lajang. Saya seorang pria berusia 30 tahun. Saya lahir di Jakarta. Saya suka musik dan olahraga. Saya lajang. Saya seorang pria berusia 30 tahun. Saya lahir di Jakarta. Saya suka musik dan olahraga. Saya lajang.'\n",
    "\n",
    "id_data2 = 'Bahasa Indonesia ([baˈhasa indoˈnesija]) merupakan bahasa resmi sekaligus bahasa nasional di Indonesia.[15] Bahasa Indonesia merupakan varietas yang dibakukan dari bahasa Melayu,[16] sebuah bahasa rumpun Austronesia yang digolongkan kedalam rumpun Melayik yang sendirinya merupakan cabang turunan dari cabang Melayu-Polinesia. Bahasa Indonesia telah sejak lama digunakan sebagai basantara di wilayah kepulauan Indonesia yang rata-rata memiliki kemajemukan linguistika. Dengan jumlah penutur bahasa yang lumayan besar ditambah dengan populasi diaspora yang tinggal di luar negeri, bahasa Indonesia masuk sebagai salah satu bahasa yang paling banyak digunakan atau dituturkan di seluruh duniosakata bahasa Indonesia juga dipengaruhi oleh beberapa bahasa lokal di wilayah kepulauan Indonesia (misalnya: bahasa Jawa, Minangkabau, Bugis, Makasar, dan lain sebagainya),[18] serta dari bahasa asing yang disebabkan oleh kontak sejarah dan keterikatan sejarah dengan bahasa lain dari wilayah lainnya.[19] Bahasa Indonesia memiliki banyak kata serapan yang berasal dari bahasa-bahasa Eropa, terutama dari bahasa Belanda, Portugis, Spanyol, dan Inggris. Bahasa Indonesia juga memiliki kata serapan yang berasal dari bahasa Sanskerta, Tionghoa, dan Arab yang membaur menjadi elemen dalam bahasa Indonesia yang terpengaruh karena adanya faktor-faktor seperti aktivitas perdagangan maupun religius yang telah berlangsung sejak zaman kuno di wilayah kepulauan Indonesia.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_func(data):\n",
    "    len_col, len_row = 45, 16100\n",
    "    data_frame = pd.DataFrame(0, index = range(len_row), columns = range(len_col))\n",
    "    \n",
    "    data = data.split()\n",
    "    data_num = [list(map(ord, list(word.lower()))) for word in data if word.isalpha()]\n",
    "    \n",
    "    for idx2, word in enumerate(data_num):\n",
    "        for idx3, i in enumerate(word):\n",
    "            data_frame.loc[idx2, idx3] = i\n",
    "        \n",
    "    data_frame = data_frame.values.reshape(-1)\n",
    "    data_frame = torch.tensor(data_frame, dtype = torch.float32)\n",
    "    \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "# - {0: 'en', 1: 'fr', 2: 'id', 3: 'tl'}\n",
    "def predict(data):\n",
    "    tensor_data = frame_func(data)\n",
    "    \n",
    "    model.eval()\n",
    "    pred = model(tensor_data)\n",
    "    pred = torch.argmax(pred)\n",
    "    if pred == 0:\n",
    "        print('영어')\n",
    "    elif pred == 1:\n",
    "        print('프랑스어')\n",
    "    elif pred == 2:\n",
    "        print('인도네시아어')\n",
    "    elif pred == 3:\n",
    "        print('필리핀어')\n",
    "    else:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어\n",
      "영어\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(new_data), predict(id_data), predict(id_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ㅠㅠ 다 영어로 나오는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
