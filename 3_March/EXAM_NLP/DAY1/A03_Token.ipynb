{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 (Tokenization)\n",
    "# - 문장을 의미 있는 최소 단위로 나누는 것\n",
    "# - 최소 단위 : 단어, 글자, 문장\n",
    "\n",
    "# [1] 자모 단위 토큰화 : 자음과 모음으로 토큰화 (jamo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "좋은 날\n",
      "ㅈㅗㅎㅇㅡㄴ ㄴㅏㄹ\n"
     ]
    }
   ],
   "source": [
    "# 자모 단위 토큰화\n",
    "import jamo\n",
    "\n",
    "# 완성형 단어 -> 조합형으로 변환\n",
    "msg = '좋은 날'\n",
    "\n",
    "result1= jamo.h2j(msg)\n",
    "print(result1)\n",
    "\n",
    "# 조합형 단어 -> 자모로 분리\n",
    "result2 = jamo.j2hcj(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 다양한 한국어 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy의 Okt\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 인스턴스 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 분석 문장 \n",
    "msg = \"오늘 저녁 실증랩 컴퓨터 성능을 테스트할 예정입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 => ['오늘', '저녁', '실증', '랩', '컴퓨터', '성능', '테스트', '예정']\n",
      "형태소 => ['오늘', '저녁', '실증', '랩', '컴퓨터', '성능', '을', '테스트', '할', '예정', '입니다', '.']\n",
      "품사 => [('오늘', 'Noun'), ('저녁', 'Noun'), ('실증', 'Noun'), ('랩', 'Noun'), ('컴퓨터', 'Noun'), ('성능', 'Noun'), ('을', 'Josa'), ('테스트', 'Noun'), ('할', 'Verb'), ('예정', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n",
      "구문 분석 => ['오늘', '오늘 저녁', '오늘 저녁 실증랩', '오늘 저녁 실증랩 컴퓨터', '오늘 저녁 실증랩 컴퓨터 성능', '테스트', '테스트할 예정', '저녁', '실증', '컴퓨터', '성능', '예정']\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(f'명사 => {okt.nouns(msg)}')\n",
    "print(f'형태소 => {okt.morphs(msg)}')\n",
    "print(f'품사 => {okt.pos(msg)}')\n",
    "print(f'구문 분석 => {okt.phrases(msg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 => ['오늘', '저녁', '실증랩', '컴퓨터', '성능', '테스트', '예정']\n",
      "형태소 => ['오늘', '저녁', '실증랩', '컴퓨터', '성능', '을', '테스트', '하', 'ㄹ', '예정', '이', 'ㅂ니다', '.']\n",
      "품사 => [('오늘', 'N'), ('저녁', 'N'), ('실증랩', 'N'), ('컴퓨터', 'N'), ('성능', 'N'), ('을', 'J'), ('테스트', 'N'), ('하', 'X'), ('ㄹ', 'E'), ('예정', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n",
      "구문 분석 => [[[('오늘', 'ncn')], [('오늘', 'mag')]], [[('저녁', 'ncn')]], [[('실증', 'ncpa'), ('랩', 'ncn')]], [[('컴퓨터', 'ncn')]], [[('성능', 'ncn'), ('을', 'jco')]], [[('테스트', 'ncpa'), ('하', 'xsva'), ('ㄹ', 'etm')]], [[('예정', 'ncn'), ('이', 'jp'), ('ㅂ니다', 'ef')], [('예정', 'ncpa'), ('이', 'jp'), ('ㅂ니다', 'ef')]], [[('.', 'sf')], [('.', 'sy')]]]\n"
     ]
    }
   ],
   "source": [
    "# hannanum\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "# 인스턴스 생성\n",
    "hannanum = Hannanum()\n",
    "\n",
    "# 분석 문장\n",
    "msg = \"오늘 저녁 실증랩 컴퓨터 성능을 테스트할 예정입니다.\"\n",
    "\n",
    "print(f'명사 => {hannanum.nouns(msg)}')\n",
    "print(f'형태소 => {hannanum.morphs(msg)}')\n",
    "print(f'품사 => {hannanum.pos(msg)}')\n",
    "print(f'구문 분석 => {hannanum.analyze(msg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK\n",
    "- 한글 미지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', 'New', 'Year', '!', 'Marry', 'Christmas', '!']\n",
      "['Happy New Year!', 'Marry Christmas!']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석\n",
    "from nltk import tokenize\n",
    "\n",
    "msg = 'Happy New Year! Marry Christmas!'\n",
    "\n",
    "# 단어 단위 형태소 .word_tokenize()\n",
    "result1 = tokenize.word_tokenize(msg)\n",
    "\n",
    "# 문장 단위 형태소 .sent_tokenize()\n",
    "result2 = tokenize.sent_tokenize(msg)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : 클래스는 대문자로 시작하고, 변수는 소문자로 시작!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happy', 'New', 'Year', '!', 'Marry', 'Christmas', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "puncToken = tokenize.WordPunctTokenizer()\n",
    "\n",
    "puncToken.tokenize(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "5\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하위 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
