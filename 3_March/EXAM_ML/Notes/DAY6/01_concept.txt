
3월 4일 : 오후 수업 (오전 시험)


[회귀 모델]
- 지금까지는 회귀 모델을 집중적으로 보았다
- 회귀는 오차가 얼마나 있고 얼마나 줄이느냐를 보는 것

[분류 모델]
- 혼동 (오차) 행렬
- : TN, FP, FN, TP로 구성
- 정확도 (accuracy) 행렬 
  : accuracy_score; 예측 중 정답을 맞힌 비율
    불균형 데이터의 경우 (사실 대부분의 경우) 정확도 신뢰성은 떨어짐
    (TP+TN)/(TP+TN+FP+FN)

[대안]
- 정밀도 (precision) 행렬 
  : precision_score; 예측 기준 (TP, FP)
- 재현율 (recall) 행렬 
  : recall_score; 실제 결과 기준 (TP, FN)
- 특이값 (F1) 행렬 : f1_score

- precision : 정밀도
- recall    : 재현율
- f1_score  : 정밀도와 재현율의 조화 수, 어느 한 쪽으로 치우치지 않는 값
 

[다중 분류]

최적화 : 목적함수(W/b)를 최대화 혹은 최소화 하는 방법
 -> 경사하강법 gradient decent : 비용cost/손실함수loss, 오차error
    경사상승법 gradient ascent : 이익profit, 점수score
 - 모델 평가 시 손실.비용함수 값이 최소가 될 때가 최적의 모델

[경사하강법]
- 에포크 : 전체 샘플을 모두 사용하는 한 번
- 배치 사이즈 : 전체 샘플을 쪼개어 나눈 크기
               기존 : 처음부터 끝까지 하나씩 연구 진행 - 오래 걸림
               - 몇 번을 나눠서 진행: 더 빠르고 즉각적으로 반응할 수 있음
               - 배치 학습 <-> 점진학습(경사하강)

# 참고 : 딥러닝은 LogR 이론을 그대로 가져감

[경사하강법 : SGD 최적화 ]
- 경사하강기법은 점차 발전해옴 (지금은 Adam을 주로 사용)
- SGD Stochastic Gradient Descent
  : 확률적 경사하강법
    분류와 회귀 모두 적용

[ SGD : parameters ]
- loss (=hinge) : 손실함수; 손실함수별 성능 차이 있음
- penalty (=l2) : 규제; 모델의 복잡도 제어
                 (사용) 과대적합이다 -> penalty를 변경해 조정!

- alpha (=0.0001) : 

# 수업 못 따라가겠다 너무 왔다갔다하고 말이 빠르다

SGD Classifier : 오차 계산에 두 가지가 필요함
SGD Regressor : 

predict()





